# System Architecture & Model Specifications

## ğŸ›ï¸ System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLIENT LAYER                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  cURL    â”‚  â”‚ Python   â”‚  â”‚ Browser  â”‚  â”‚  Batch   â”‚            â”‚
â”‚  â”‚ Request  â”‚  â”‚ Requests â”‚  â”‚ (Swagger)â”‚  â”‚ Scripts  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          API LAYER                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    FastAPI Application                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚
â”‚  â”‚  â”‚ POST /evaluateâ”‚  â”‚ GET /health  â”‚  â”‚  GET /docs   â”‚        â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
â”‚  â”‚         â”‚                                                      â”‚  â”‚
â”‚  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚         â””â”€â–¶â”‚  Request Validation (Pydantic)      â”‚            â”‚  â”‚
â”‚  â”‚            â”‚  - File type check (.pptx)          â”‚            â”‚  â”‚
â”‚  â”‚            â”‚  - Size limit (50MB)                â”‚            â”‚  â”‚
â”‚  â”‚            â”‚  - Problem statement validation     â”‚            â”‚  â”‚
â”‚  â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ORCHESTRATION LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Evaluation Orchestrator                           â”‚  â”‚
â”‚  â”‚  - Lazy model loading                                          â”‚  â”‚
â”‚  â”‚  - Pipeline coordination                                       â”‚  â”‚
â”‚  â”‚  - Score combination (70% LLM + 30% Semantic)                 â”‚  â”‚
â”‚  â”‚  - Schema validation                                           â”‚  â”‚
â”‚  â”‚  - Error handling & logging                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚           â”‚           â”‚           â”‚
       â–¼           â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PROCESSING LAYER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   PPT    â”‚  â”‚Summarize â”‚  â”‚ Semantic â”‚  â”‚   LLM    â”‚             â”‚
â”‚  â”‚Extractor â”‚  â”‚  Engine  â”‚  â”‚  Scorer  â”‚  â”‚Evaluator â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MODEL LAYER                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ python-  â”‚  â”‚   BART   â”‚  â”‚ MiniLM   â”‚  â”‚ Mistral  â”‚             â”‚
â”‚  â”‚   pptx   â”‚  â”‚Large-CNN â”‚  â”‚  L6-v2   â”‚  â”‚   7B     â”‚             â”‚
â”‚  â”‚          â”‚  â”‚  406M    â”‚  â”‚   22M    â”‚  â”‚  (8-bit) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                        â”‚
â”‚  Fallback: FLAN-T5-Base (250M) for CPU-only environments             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Upload â”‚
â”‚ - PPT File  â”‚
â”‚ - Problem   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. PPT Extraction   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ Input: .pptx file   â”‚
â”‚ Output:             â”‚
â”‚ {                   â”‚
â”‚   slides: [         â”‚
â”‚     {               â”‚
â”‚       title: str    â”‚
â”‚       content: []   â”‚
â”‚       notes: str    â”‚
â”‚     }               â”‚
â”‚   ]                 â”‚
â”‚ }                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Summarization    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ Input: Slide data   â”‚
â”‚ Process:            â”‚
â”‚ - Per-slide summary â”‚
â”‚ - Meta-summary      â”‚
â”‚ Output:             â”‚
â”‚ "Slide 1: ...       â”‚
â”‚  Slide 2: ..."      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                          â”‚
       â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3a. Semantic    â”‚    â”‚ 3b. LLM Evaluation  â”‚
â”‚     Scoring     â”‚    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    â”‚ Input:              â”‚
â”‚ Input:          â”‚    â”‚ - Problem statement â”‚
â”‚ - Problem       â”‚    â”‚ - Summary           â”‚
â”‚ - Summary       â”‚    â”‚ Process:            â”‚
â”‚ Process:        â”‚    â”‚ - Structured prompt â”‚
â”‚ - Embed both    â”‚    â”‚ - JSON generation   â”‚
â”‚ - Cosine sim    â”‚    â”‚ - Parse & validate  â”‚
â”‚ Output:         â”‚    â”‚ Output:             â”‚
â”‚ relevance: 8.2  â”‚    â”‚ {                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   scores: {...}     â”‚
       â”‚               â”‚   strengths: [...]  â”‚
       â”‚               â”‚   weaknesses: [...] â”‚
       â”‚               â”‚ }                   â”‚
       â”‚               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Score Combination        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ adjusted_relevance =        â”‚
â”‚   0.7 * llm_score +         â”‚
â”‚   0.3 * semantic_score      â”‚
â”‚                             â”‚
â”‚ overall_score =             â”‚
â”‚   sum(all_dimension_scores) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Validation       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ - Pydantic schema   â”‚
â”‚ - Score ranges      â”‚
â”‚ - Required fields   â”‚
â”‚ - Type checking     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON Response       â”‚
â”‚ {                   â”‚
â”‚   scores: {...}     â”‚
â”‚   overall_score: 40 â”‚
â”‚   strengths: [...]  â”‚
â”‚   weaknesses: [...] â”‚
â”‚   ...               â”‚
â”‚ }                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤– Model Specifications

### 1. Summarization Model

**Model ID**: `facebook/bart-large-cnn`

**Specifications**:
- **Architecture**: BART (Bidirectional and Auto-Regressive Transformers)
- **Parameters**: 406 million
- **Training Data**: CNN/DailyMail dataset (news articles)
- **Max Input**: 1024 tokens
- **Max Output**: 142 tokens (configurable)
- **License**: Apache 2.0

**Performance**:
- CPU: 2-3 seconds per slide
- GPU: <1 second per slide
- Memory: ~1.6GB (model weights)

**Why This Model**:
- State-of-the-art abstractive summarization
- Excellent at condensing verbose content
- Trained on news articles (similar to presentation content)
- Good balance of quality and speed

**Configuration**:
```python
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=0  # GPU, or -1 for CPU
)

summary = summarizer(
    text,
    max_length=130,
    min_length=30,
    do_sample=False
)
```

---

### 2. Embedding Model

**Model ID**: `sentence-transformers/all-MiniLM-L6-v2`

**Specifications**:
- **Architecture**: MiniLM (distilled from RoBERTa)
- **Parameters**: 22 million
- **Embedding Dimensions**: 384
- **Max Sequence Length**: 256 tokens
- **Training**: Trained on 1B+ sentence pairs
- **License**: Apache 2.0

**Performance**:
- Speed: ~1000 sentences/second on CPU
- Memory: ~90MB (model weights)
- Inference: <100ms for full presentation

**Why This Model**:
- Best speed/quality tradeoff in sentence-transformers
- Optimized specifically for semantic similarity
- Very fast on CPU (no GPU needed)
- Compact 384-dim embeddings

**Configuration**:
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode([text1, text2])
similarity = cosine_similarity(embeddings[0], embeddings[1])
```

---

### 3. LLM Evaluation Model (Primary)

**Model ID**: `mistralai/Mistral-7B-Instruct-v0.2`

**Specifications**:
- **Architecture**: Mistral (decoder-only transformer)
- **Parameters**: 7 billion
- **Context Length**: 8192 tokens
- **Quantization**: 8-bit (optional, reduces VRAM by 50%)
- **Training**: Instruction-tuned on diverse tasks
- **License**: Apache 2.0

**Performance**:
- GPU (8-bit): 3-5 seconds, 7GB VRAM
- GPU (FP16): 2-3 seconds, 14GB VRAM
- CPU: 30-60 seconds, 16GB RAM

**Why This Model**:
- Excellent instruction following
- Strong JSON generation capabilities
- 8K context handles long presentations
- Apache 2.0 license (commercial use OK)
- Active community and support

**Configuration**:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.2",
    load_in_8bit=True,  # 8-bit quantization
    device_map="auto"
)

outputs = model.generate(
    inputs,
    max_new_tokens=1024,
    temperature=0.3,
    top_p=0.9
)
```

---

### 4. LLM Evaluation Model (Fallback)

**Model ID**: `google/flan-t5-base`

**Specifications**:
- **Architecture**: T5 (encoder-decoder)
- **Parameters**: 250 million
- **Context Length**: 512 tokens
- **Training**: Instruction-tuned on 1800+ tasks
- **License**: Apache 2.0

**Performance**:
- CPU: 10-15 seconds
- GPU: 1-2 seconds
- Memory: ~1GB

**Why This Model**:
- CPU-friendly (250M vs 7B params)
- Good instruction following
- Fast inference
- Reliable JSON generation

**When Used**:
- Automatically selected on CPU if Mistral is too large
- Manual override in config.py
- Resource-constrained environments

**Configuration**:
```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

outputs = model.generate(
    inputs,
    max_new_tokens=512,
    temperature=0.3
)
```

---

## ğŸ”„ Model Selection Decision Tree

```
Start: Evaluate Presentation
â”‚
â”œâ”€ Is GPU available?
â”‚  â”‚
â”‚  â”œâ”€ YES
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ VRAM >= 14GB?
â”‚  â”‚  â”‚  â”œâ”€ YES â†’ Use Mistral-7B (FP16)
â”‚  â”‚  â”‚  â””â”€ NO â†’ VRAM >= 7GB?
â”‚  â”‚  â”‚         â”œâ”€ YES â†’ Use Mistral-7B (8-bit)
â”‚  â”‚  â”‚         â””â”€ NO â†’ Use FLAN-T5-Base
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Summarization: BART-Large-CNN (GPU)
â”‚  â”‚     Embeddings: MiniLM-L6-v2
â”‚  â”‚
â”‚  â””â”€ NO (CPU only)
â”‚     â”‚
â”‚     â”œâ”€ RAM >= 16GB?
â”‚     â”‚  â”œâ”€ YES â†’ Use Mistral-7B (slow but accurate)
â”‚     â”‚  â””â”€ NO â†’ Use FLAN-T5-Base (fast, good quality)
â”‚     â”‚
â”‚     â””â”€ Summarization: BART-Large-CNN (CPU)
â”‚        Embeddings: MiniLM-L6-v2
â”‚
End: Return Evaluation
```

---

## ğŸ“¦ Model Download & Caching

### Automatic Download

All models are automatically downloaded on first use:

```python
# Models cached to: ~/.cache/huggingface/hub/
# Typical sizes:
# - BART-Large-CNN: ~1.6GB
# - MiniLM-L6-v2: ~90MB
# - Mistral-7B: ~14GB (FP16) or ~7GB (8-bit)
# - FLAN-T5-Base: ~1GB
```

### Pre-download (Optional)

```python
from transformers import AutoModel
from sentence_transformers import SentenceTransformer

# Download all models
AutoModel.from_pretrained("facebook/bart-large-cnn")
SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
AutoModel.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
AutoModel.from_pretrained("google/flan-t5-base")
```

### Custom Cache Directory

```python
# In config.py
import os
os.environ['TRANSFORMERS_CACHE'] = '/path/to/cache'
```

---

## âš¡ Performance Optimization Strategies

### 1. 8-bit Quantization (GPU)

```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # Reduces VRAM by 50%
    device_map="auto"
)
```

**Impact**:
- VRAM: 14GB â†’ 7GB
- Speed: ~10% slower
- Quality: <1% degradation

### 2. Batch Processing

```python
# Summarize multiple slides at once
summaries = summarizer(
    [slide1, slide2, slide3],
    batch_size=8
)
```

**Impact**:
- Speed: 2-3x faster for large presentations
- Memory: Slightly higher peak usage

### 3. Model Pruning (Future)

```python
# Remove less important weights
from transformers import prune_linear_layer

pruned_model = prune_linear_layer(model, amount=0.3)
```

**Impact**:
- Size: 30% smaller
- Speed: 20-30% faster
- Quality: 2-5% degradation

---

## ğŸ¯ Model Comparison Matrix

| Aspect | BART-Large | MiniLM-L6 | Mistral-7B | FLAN-T5 |
|--------|-----------|-----------|------------|---------|
| **Task** | Summarize | Embed | Evaluate | Evaluate (fallback) |
| **Params** | 406M | 22M | 7B | 250M |
| **Speed (GPU)** | Fast | Very Fast | Medium | Fast |
| **Speed (CPU)** | Medium | Very Fast | Slow | Medium |
| **Quality** | Excellent | Excellent | Excellent | Good |
| **VRAM (GPU)** | 2GB | <1GB | 7-14GB | 1GB |
| **RAM (CPU)** | 4GB | <1GB | 16GB | 2GB |
| **License** | Apache 2.0 | Apache 2.0 | Apache 2.0 | Apache 2.0 |

---

## ğŸ”§ Troubleshooting Model Issues

### Issue: Model Download Fails

**Cause**: Network issues, Hugging Face rate limiting

**Solution**:
```bash
# Use mirror
export HF_ENDPOINT=https://hf-mirror.com

# Or download manually
huggingface-cli download facebook/bart-large-cnn
```

### Issue: Out of Memory

**Cause**: GPU VRAM or system RAM insufficient

**Solution**:
```python
# 1. Enable 8-bit quantization
USE_8BIT = True

# 2. Use smaller models
LLM_MODEL = "google/flan-t5-base"
SUMMARIZATION_MODEL = "facebook/bart-base"

# 3. Reduce batch size
batch_size = 1
```

### Issue: Slow Inference

**Cause**: Running on CPU or large model

**Solution**:
```python
# 1. Use GPU if available
DEVICE = "cuda"

# 2. Use smaller models
LLM_MODEL = "google/flan-t5-base"

# 3. Enable optimizations
torch.backends.cudnn.benchmark = True
```

---

## ğŸ“Š Recommended Hardware Configurations

### Minimum (CPU Only)

- **CPU**: 4 cores, 2.5GHz+
- **RAM**: 8GB
- **Storage**: 10GB
- **Models**: FLAN-T5-Base, BART-Base
- **Performance**: ~60s per evaluation

### Recommended (Consumer GPU)

- **GPU**: RTX 3060 (12GB VRAM) or better
- **CPU**: 6 cores
- **RAM**: 16GB
- **Storage**: 20GB
- **Models**: Mistral-7B (8-bit), BART-Large
- **Performance**: ~5-8s per evaluation

### Optimal (Professional GPU)

- **GPU**: RTX 4090 (24GB VRAM) or A100
- **CPU**: 8+ cores
- **RAM**: 32GB
- **Storage**: 50GB
- **Models**: Mistral-7B (FP16), all large variants
- **Performance**: ~3-5s per evaluation

---

**This architecture is designed for production use with automatic fallbacks, comprehensive error handling, and optimal performance across diverse hardware configurations.**
